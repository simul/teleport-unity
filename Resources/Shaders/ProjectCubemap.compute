
// (c) 2020 Simul.co

#pragma kernel EncodeTagDataIdCS
#pragma kernel EncodeColorCS
#pragma kernel EncodeCubemapFaceCS
#pragma kernel EncodeWebcamCS

#include "Common.cginc"
// Conventional layout
static const int2 FaceOffsets[] = { {0,0},{1,0},{2,0},{0,1},{1,1},{2,1} };
// GL Layout
//static const int2 FaceOffsets[] = { {2,1},{1,1},{0,0},{1,0},{2,0},{0,1} };

Texture2D<float4> InputColorTexture;
Texture2D<float4> InputDepthTexture;
TextureCube<float4> InputCubemapTexture;
SamplerState samplerInputColorTexture;
SamplerState samplerInputCubemapTexture;
RWTexture2D<float4> RWOutputColorTexture;
RWTexture2DArray<float4> RWOutputColorTextureArray;

int2 Offset;
int2 WebcamSize;
int2 TagDataIdOffset;
int TagDataId;
int Face;
//mat4 FaceToWorldMatrix;

// Here. we will encode the id of the video tag data in 4x4 blocks of monochrome colour.
[numthreads(32, 4, 1)]
void EncodeTagDataIdCS(uint2 ThreadID : SV_DispatchThreadID)
{
	uint OutputW, OutputH;
	RWOutputColorTexture.GetDimensions(OutputW, OutputH);

	// We want to encode one 32-bit number with the minimum possible loss.
	// Therefore we will encode it in...
	// Binary.
	uint raw_uint = asuint(TagDataId);

	// We will use the thread x as the bit index.
	uint masked_bits = (raw_uint >> (ThreadID.x / 4))& uint(1);		// 1 or 0
	int2 Pos = TagDataIdOffset + int2(ThreadID);
	RWOutputColorTexture[Pos] = float4(masked_bits.x, masked_bits.x, masked_bits.x, masked_bits.x);
}

float PosToDistanceMultiplier(int2 pos, int w)
{
	float h = (w + 1) / 2.0;
	vec2 diff = (vec2(pos) - vec2(h, h)) * 2.0 / vec2(w, w);
	return sqrt(1.0 + dot(diff, diff));
}

float GetDepth(int2 pos, int w)
{
	float m = PosToDistanceMultiplier(pos, w);
	float d = (LinearEyeDepth(InputDepthTexture[pos].r) - 5.0) / 25.0;
	d *= m;
	return d;
}

[numthreads(THREADGROUP_SIZEX, THREADGROUP_SIZEY, 1)]
void EncodeColorCS(uint2 ThreadID : SV_DispatchThreadID)
{
	uint InputW, InputH;
	InputColorTexture.GetDimensions(InputW, InputH);

	if (ThreadID.x>=InputW || ThreadID.y>=InputH)
		return;

	int2 pos = int2(ThreadID);
	int2 ipos = int2(pos.x, InputH - 1 - pos.y);
	float4 SceneColor = InputColorTexture[ipos];
	SceneColor.x = sqrt(SceneColor.x);
	SceneColor.y = sqrt(SceneColor.y);
	SceneColor.z = sqrt(SceneColor.z);
	SceneColor.w = GetDepth(ipos, int(InputW));
	pos = pos.xy + Offset + FaceOffsets[Face] * InputW;

	RWOutputColorTexture[pos] = SceneColor;
}

[numthreads(THREADGROUP_SIZEX, THREADGROUP_SIZEY, 1)]
void EncodeCubemapFaceCS(uint2 ThreadID : SV_DispatchThreadID)
{
	uint InputW, InputH, numFaces;
	InputCubemapTexture.GetDimensions(InputW, InputH);

	if (ThreadID.x >= InputW || ThreadID.y >= InputH)
		return;

	int2 pos = int2(ThreadID);
	int4 ipos = int4(pos.x, InputH - 1 - pos.y,Face,0);
	// we must convert Face index and pos into a 3D direction, to get around Unity's API limitations.
	float3 dir=float3(float2(ipos.xy)*2.0-float2(1.0,1.0),1.0);
	// A MATRIX TRANSFORMATION, just to extract a pixel value that we should be able to look up directly...
	// use FaceToWorldMatrix.
	
	dir = float3(-dir.z, dir.x, dir.y);
	float4 SceneColor = InputCubemapTexture.SampleLevel(samplerInputCubemapTexture,normalize(dir),0);
	SceneColor.x = sqrt(SceneColor.x);
	SceneColor.y = sqrt(SceneColor.y);
	SceneColor.z = sqrt(SceneColor.z);

	pos += Offset+FaceOffsets[Face] * InputW;

	RWOutputColorTexture[pos] = SceneColor;
}

[numthreads(THREADGROUP_SIZEX, THREADGROUP_SIZEY, 1)]
void EncodeWebcamCS(uint2 ThreadID : SV_DispatchThreadID)
{
	if (int(ThreadID.x) >= WebcamSize.x || int(ThreadID.y) >= WebcamSize.y)
		return;

	//int2 div = int2(InputW, InputH) / int2(WebcamSize.x, WebcamSize.y);

	float2 uv = float2(ThreadID) / float2(WebcamSize);

	// Flip for HLSL
	uv.y = 1.0 - uv.y;

	float4 color = InputColorTexture.SampleLevel(samplerInputColorTexture, uv, 0);
	color.x = sqrt(color.x);
	color.y = sqrt(color.y);
	color.z = sqrt(color.z);

	int2 pos = Offset + int2(ThreadID);

	RWOutputColorTexture[pos] = color;
}
